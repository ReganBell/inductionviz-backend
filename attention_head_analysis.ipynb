{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head Analysis\n",
    "\n",
    "This notebook analyzes the attention heads in the attn-only-1l and attn-only-2l models.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### OV and QK Matrices\n",
    "- **OV Matrix**: W_V @ W_O - maps from source token embedding to output effect on logits\n",
    "- **QK Matrix**: W_Q @ W_K^T - determines attention patterns (which queries attend to which keys)\n",
    "\n",
    "### Heuristic Decisions\n",
    "\n",
    "1. **Pivot on Key**: We pivot the table on the key (source) token. This means each row shows one key and its corresponding queries and outputs. The key is shared between QK and OV matrices.\n",
    "\n",
    "2. **QK Normalization**: QK values are normalized by subtracting the QK value for the special BOS token (token 50256) for each query, since models use it as a default value. This makes QK values comparable across different queries.\n",
    "\n",
    "3. **OV Normalization**: OV values are normalized by subtracting the mean to make values consistent across different source tokens.\n",
    "\n",
    "4. **Key Selection**: Keys are selected using the heuristic: `QK.max(0) * OV.max(0) * token_prob**0.1`. This favors:\n",
    "   - Keys with queries that strongly prefer them (high QK)\n",
    "   - Keys with large effect on output (high OV)\n",
    "   - Slightly prefers probable keys (they occur more frequently)\n",
    "\n",
    "5. **Query Selection**: Queries are selected by `QK[:, src] * token_prob**0.1` - upweighting probable tokens since they're more likely to occur.\n",
    "\n",
    "6. **Output Selection**: Output tokens are simply those with the largest OV values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformer_lens import HookedTransformer\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "model_1l = HookedTransformer.from_pretrained(\"attn-only-1l\", device=DEVICE).eval()\n",
    "model_2l = HookedTransformer.from_pretrained(\"attn-only-2l\", device=DEVICE).eval()\n",
    "\n",
    "# Load tokenizer\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "VOCAB_SIZE = enc.n_vocab\n",
    "BOS_TOKEN_ID = enc.n_vocab  # GPT-2 uses vocab_size as BOS\n",
    "\n",
    "print(f\"1L Model: {model_1l.cfg.n_layers} layers, {model_1l.cfg.n_heads} heads, d_model={model_1l.cfg.d_model}\")\n",
    "print(f\"2L Model: {model_2l.cfg.n_layers} layers, {model_2l.cfg.n_heads} heads, d_model={model_2l.cfg.d_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word frequency data for token probabilities\n",
    "def load_token_frequencies():\n",
    "    \"\"\"Load token frequencies from words.txt to estimate token probabilities.\"\"\"\n",
    "    try:\n",
    "        with open('words.txt', 'r') as f:\n",
    "            content = f.read()\n",
    "            token_counts = defaultdict(int)\n",
    "            total_tokens = 0\n",
    "            \n",
    "            for line in content.split('\\n'):\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                ids = enc.encode(line)\n",
    "                for tid in ids:\n",
    "                    token_counts[tid] += 1\n",
    "                    total_tokens += 1\n",
    "            \n",
    "            # Convert to probabilities\n",
    "            token_probs = torch.zeros(VOCAB_SIZE)\n",
    "            for tid, count in token_counts.items():\n",
    "                token_probs[tid] = count / total_tokens\n",
    "            \n",
    "            # Add small smoothing for unseen tokens\n",
    "            token_probs = token_probs + 1e-10\n",
    "            token_probs = token_probs / token_probs.sum()\n",
    "            \n",
    "            print(f\"Loaded frequencies for {len(token_counts)} unique tokens from {total_tokens} total tokens\")\n",
    "            return token_probs\n",
    "    except FileNotFoundError:\n",
    "        print(\"words.txt not found, using uniform distribution\")\n",
    "        return torch.ones(VOCAB_SIZE) / VOCAB_SIZE\n",
    "\n",
    "token_probs = load_token_frequencies().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def compute_qk_ov_matrices(model: HookedTransformer, layer: int):\n",
    "    \"\"\"\n",
    "    Compute QK and OV matrices for all heads in a layer.\n",
    "    \n",
    "    Returns:\n",
    "        qk_matrices: [n_heads, d_vocab, d_vocab] - QK effect via embedding\n",
    "        ov_matrices: [n_heads, d_vocab, d_vocab] - OV effect via unembedding\n",
    "    \"\"\"\n",
    "    n_heads = model.cfg.n_heads\n",
    "    d_vocab = model.cfg.d_vocab\n",
    "    \n",
    "    # Get weight matrices\n",
    "    W_Q = model.W_Q[layer]  # [n_heads, d_model, d_head]\n",
    "    W_K = model.W_K[layer]  # [n_heads, d_model, d_head]\n",
    "    W_V = model.W_V[layer]  # [n_heads, d_model, d_head]\n",
    "    W_O = model.W_O[layer]  # [n_heads, d_head, d_model]\n",
    "    W_E = model.W_E         # [d_vocab, d_model]\n",
    "    W_U = model.W_U         # [d_model, d_vocab]\n",
    "    \n",
    "    # Compute QK circuit: W_Q @ W_K^T\n",
    "    # For each head: [d_model, d_head] @ [d_head, d_model] = [d_model, d_model]\n",
    "    W_QK = torch.einsum('hqi,hki->hqk', W_Q, W_K)  # [n_heads, d_model, d_model]\n",
    "    \n",
    "    # Compute OV circuit: W_V @ W_O\n",
    "    # For each head: [d_model, d_head] @ [d_head, d_model] = [d_model, d_model]\n",
    "    W_OV = torch.einsum('hvi,hio->hvo', W_V, W_O)  # [n_heads, d_model, d_model]\n",
    "    \n",
    "    # Project through embedding and unembedding\n",
    "    # QK in token space: [d_vocab, d_model] @ [d_model, d_model] @ [d_model, d_vocab]\n",
    "    qk_matrices = torch.einsum('vq,hqk,kw->hvw', W_E, W_QK, W_E.T)  # [n_heads, d_vocab, d_vocab]\n",
    "    \n",
    "    # OV in token space: [d_vocab, d_model] @ [d_model, d_model] @ [d_model, d_vocab]\n",
    "    ov_matrices = torch.einsum('vs,hso,ow->hvw', W_E, W_OV, W_U)  # [n_heads, d_vocab, d_vocab]\n",
    "    \n",
    "    return qk_matrices, ov_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def normalize_qk_ov(qk_matrices, ov_matrices, bos_token_id: int):\n",
    "    \"\"\"\n",
    "    Normalize QK and OV matrices according to the heuristics.\n",
    "    \n",
    "    - QK: Subtract the QK value for BOS token for each query\n",
    "    - OV: Subtract the mean for each source token\n",
    "    \"\"\"\n",
    "    qk_normalized = qk_matrices.clone()\n",
    "    ov_normalized = ov_matrices.clone()\n",
    "    \n",
    "    # Normalize QK: subtract BOS baseline for each query\n",
    "    # qk_matrices is [n_heads, query, key]\n",
    "    # For each query, subtract qk_matrices[:, query, bos_token_id]\n",
    "    bos_baseline = qk_matrices[:, :, bos_token_id:bos_token_id+1]  # [n_heads, d_vocab, 1]\n",
    "    qk_normalized = qk_matrices - bos_baseline\n",
    "    \n",
    "    # Normalize OV: subtract mean for each source token\n",
    "    # ov_matrices is [n_heads, source, output]\n",
    "    ov_mean = ov_matrices.mean(dim=-1, keepdim=True)  # [n_heads, d_vocab, 1]\n",
    "    ov_normalized = ov_matrices - ov_mean\n",
    "    \n",
    "    return qk_normalized, ov_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_interesting_keys(\n",
    "    qk_normalized: torch.Tensor,\n",
    "    ov_normalized: torch.Tensor,\n",
    "    token_probs: torch.Tensor,\n",
    "    head_idx: int,\n",
    "    n_keys: int = 20,\n",
    ") -> List[int]:\n",
    "    \"\"\"\n",
    "    Select interesting keys using the heuristic:\n",
    "    QK.max(0) * OV.max(0) * token_prob**0.1\n",
    "    \n",
    "    Args:\n",
    "        qk_normalized: [n_heads, query, key]\n",
    "        ov_normalized: [n_heads, source, output]\n",
    "        token_probs: [d_vocab]\n",
    "        head_idx: Which head to analyze\n",
    "        n_keys: Number of top keys to return\n",
    "    \"\"\"\n",
    "    # Get max QK value across all queries for each key\n",
    "    max_qk_per_key = qk_normalized[head_idx].max(dim=0).values  # [d_vocab]\n",
    "    \n",
    "    # Get max OV value across all outputs for each source\n",
    "    max_ov_per_source = ov_normalized[head_idx].max(dim=1).values  # [d_vocab]\n",
    "    \n",
    "    # Compute importance score\n",
    "    importance = max_qk_per_key * max_ov_per_source * (token_probs ** 0.1)\n",
    "    \n",
    "    # Get top keys\n",
    "    top_key_indices = torch.topk(importance, k=min(n_keys, len(importance))).indices\n",
    "    \n",
    "    return top_key_indices.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_queries_and_outputs(\n",
    "    qk_normalized: torch.Tensor,\n",
    "    ov_normalized: torch.Tensor,\n",
    "    token_probs: torch.Tensor,\n",
    "    head_idx: int,\n",
    "    key_idx: int,\n",
    "    n_queries: int = 20,\n",
    "    n_outputs: int = 20,\n",
    ") -> Tuple[List[Tuple[int, float]], List[Tuple[int, float]]]:\n",
    "    \"\"\"\n",
    "    Get top queries and outputs for a given key.\n",
    "    \n",
    "    Queries: sorted by QK[:, key] * token_prob**0.1\n",
    "    Outputs: sorted by OV[key, :]\n",
    "    \n",
    "    Returns:\n",
    "        top_queries: List of (token_id, score)\n",
    "        top_outputs: List of (token_id, score)\n",
    "    \"\"\"\n",
    "    # Get QK scores for all queries attending to this key\n",
    "    qk_scores = qk_normalized[head_idx, :, key_idx]  # [d_vocab]\n",
    "    query_importance = qk_scores * (token_probs ** 0.1)\n",
    "    \n",
    "    # Get top queries\n",
    "    top_query_indices = torch.topk(query_importance, k=min(n_queries, len(query_importance))).indices\n",
    "    top_queries = [(idx.item(), qk_scores[idx].item()) for idx in top_query_indices]\n",
    "    \n",
    "    # Get OV scores for all output effects from this source\n",
    "    ov_scores = ov_normalized[head_idx, key_idx, :]  # [d_vocab]\n",
    "    \n",
    "    # Get top outputs\n",
    "    top_output_indices = torch.topk(ov_scores, k=min(n_outputs, len(ov_scores))).indices\n",
    "    top_outputs = [(idx.item(), ov_scores[idx].item()) for idx in top_output_indices]\n",
    "    \n",
    "    return top_queries, top_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_token(token_id: int) -> str:\n",
    "    \"\"\"Format a token for display, handling special characters.\"\"\"\n",
    "    token_str = enc.decode([token_id])\n",
    "    # Escape special characters for display\n",
    "    token_str = repr(token_str)[1:-1]  # Remove outer quotes from repr\n",
    "    return f\"'{token_str}'\"\n",
    "\n",
    "def print_head_analysis(\n",
    "    model_name: str,\n",
    "    layer: int,\n",
    "    head: int,\n",
    "    qk_normalized: torch.Tensor,\n",
    "    ov_normalized: torch.Tensor,\n",
    "    token_probs: torch.Tensor,\n",
    "    n_keys: int = 5,\n",
    "    n_queries: int = 20,\n",
    "    n_outputs: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Print formatted analysis for a single attention head.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Head {layer}:{head} ({model_name})\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Select interesting keys\n",
    "    interesting_keys = select_interesting_keys(\n",
    "        qk_normalized, ov_normalized, token_probs, head, n_keys=n_keys\n",
    "    )\n",
    "    \n",
    "    for key_idx in interesting_keys:\n",
    "        key_token = format_token(key_idx)\n",
    "        \n",
    "        # Get top queries and outputs\n",
    "        top_queries, top_outputs = get_top_queries_and_outputs(\n",
    "            qk_normalized, ov_normalized, token_probs,\n",
    "            head, key_idx, n_queries, n_outputs\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'─'*80}\")\n",
    "        print(f\"Key: {key_token}\")\n",
    "        print(f\"{'─'*80}\")\n",
    "        \n",
    "        # Print queries that prefer this key\n",
    "        print(\"\\nQueries that prefer key:\")\n",
    "        query_strs = [f\"{format_token(tid)} ({score:.2f})\" for tid, score in top_queries]\n",
    "        # Print in rows of ~80 chars\n",
    "        current_line = \"\"\n",
    "        for qs in query_strs:\n",
    "            if len(current_line) + len(qs) + 2 > 78:\n",
    "                print(current_line)\n",
    "                current_line = qs\n",
    "            else:\n",
    "                if current_line:\n",
    "                    current_line += \" \" + qs\n",
    "                else:\n",
    "                    current_line = qs\n",
    "        if current_line:\n",
    "            print(current_line)\n",
    "        \n",
    "        # Print effect on logits\n",
    "        print(\"\\nEffect on logits:\")\n",
    "        output_strs = [f\"{format_token(tid)} ({score:.2f})\" for tid, score in top_outputs]\n",
    "        current_line = \"\"\n",
    "        for os in output_strs:\n",
    "            if len(current_line) + len(os) + 2 > 78:\n",
    "                print(current_line)\n",
    "                current_line = os\n",
    "            else:\n",
    "                if current_line:\n",
    "                    current_line += \" \" + os\n",
    "                else:\n",
    "                    current_line = os\n",
    "        if current_line:\n",
    "            print(current_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: 1-Layer Model (attn-only-1l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute QK and OV matrices for 1L model\n",
    "print(\"Computing QK and OV matrices for 1-layer model...\")\n",
    "qk_1l_L0, ov_1l_L0 = compute_qk_ov_matrices(model_1l, layer=0)\n",
    "\n",
    "# Normalize\n",
    "qk_1l_L0_norm, ov_1l_L0_norm = normalize_qk_ov(qk_1l_L0, ov_1l_L0, BOS_TOKEN_ID)\n",
    "\n",
    "print(f\"QK matrix shape: {qk_1l_L0_norm.shape}\")\n",
    "print(f\"OV matrix shape: {ov_1l_L0_norm.shape}\")\n",
    "print(f\"Number of heads: {model_1l.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each head in layer 0\n",
    "for head in range(model_1l.cfg.n_heads):\n",
    "    print_head_analysis(\n",
    "        \"attn-only-1l\",\n",
    "        layer=0,\n",
    "        head=head,\n",
    "        qk_normalized=qk_1l_L0_norm,\n",
    "        ov_normalized=ov_1l_L0_norm,\n",
    "        token_probs=token_probs,\n",
    "        n_keys=5,  # Show 5 interesting keys per head\n",
    "        n_queries=20,\n",
    "        n_outputs=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: 2-Layer Model (attn-only-2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute QK and OV matrices for 2L model\n",
    "print(\"Computing QK and OV matrices for 2-layer model...\")\n",
    "qk_2l_L0, ov_2l_L0 = compute_qk_ov_matrices(model_2l, layer=0)\n",
    "qk_2l_L1, ov_2l_L1 = compute_qk_ov_matrices(model_2l, layer=1)\n",
    "\n",
    "# Normalize\n",
    "qk_2l_L0_norm, ov_2l_L0_norm = normalize_qk_ov(qk_2l_L0, ov_2l_L0, BOS_TOKEN_ID)\n",
    "qk_2l_L1_norm, ov_2l_L1_norm = normalize_qk_ov(qk_2l_L1, ov_2l_L1, BOS_TOKEN_ID)\n",
    "\n",
    "print(f\"Layer 0 - QK matrix shape: {qk_2l_L0_norm.shape}\")\n",
    "print(f\"Layer 0 - OV matrix shape: {ov_2l_L0_norm.shape}\")\n",
    "print(f\"Layer 1 - QK matrix shape: {qk_2l_L1_norm.shape}\")\n",
    "print(f\"Layer 1 - OV matrix shape: {ov_2l_L1_norm.shape}\")\n",
    "print(f\"Number of heads per layer: {model_2l.cfg.n_heads}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each head in layer 0\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# LAYER 0\")\n",
    "print(\"#\"*80)\n",
    "for head in range(model_2l.cfg.n_heads):\n",
    "    print_head_analysis(\n",
    "        \"attn-only-2l\",\n",
    "        layer=0,\n",
    "        head=head,\n",
    "        qk_normalized=qk_2l_L0_norm,\n",
    "        ov_normalized=ov_2l_L0_norm,\n",
    "        token_probs=token_probs,\n",
    "        n_keys=5,\n",
    "        n_queries=20,\n",
    "        n_outputs=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each head in layer 1\n",
    "print(\"\\n\" + \"#\"*80)\n",
    "print(\"# LAYER 1\")\n",
    "print(\"#\"*80)\n",
    "for head in range(model_2l.cfg.n_heads):\n",
    "    print_head_analysis(\n",
    "        \"attn-only-2l\",\n",
    "        layer=1,\n",
    "        head=head,\n",
    "        qk_normalized=qk_2l_L1_norm,\n",
    "        ov_normalized=ov_2l_L1_norm,\n",
    "        token_probs=token_probs,\n",
    "        n_keys=5,\n",
    "        n_queries=20,\n",
    "        n_outputs=20,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Guide\n",
    "\n",
    "For each attention head, we show:\n",
    "\n",
    "1. **Key**: The source token being analyzed\n",
    "2. **Queries that prefer key**: Query tokens that would strongly attend to this key (with normalized QK scores)\n",
    "3. **Effect on logits**: Output tokens that are promoted when attending to this key (with normalized OV values)\n",
    "\n",
    "### What to look for:\n",
    "\n",
    "- **Induction heads**: Look for heads where queries after token X prefer keys at token X, and the output promotes the token that followed X previously\n",
    "- **Previous token heads**: Heads that attend to the previous token (queries prefer keys at position -1)\n",
    "- **Positional heads**: Heads with consistent attention patterns based on relative positions\n",
    "- **Syntactic heads**: Heads that attend to specific syntactic patterns (e.g., matching brackets, quotes)\n",
    "\n",
    "### Scores:\n",
    "\n",
    "- **QK scores**: Higher = query strongly prefers this key\n",
    "- **OV scores**: Higher = this source token strongly promotes this output token\n",
    "\n",
    "The normalization makes these scores comparable across different queries and keys."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
